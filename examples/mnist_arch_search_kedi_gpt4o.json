{
  "seed": 0,
  "pipeline_mode": "phased",
  "experiment_name": "mnist_arch_search_kedi_gpt4o",
  "generated_files_path": "./generated",
  "data_provider_path": "./src/mimarsinan/data_handling/data_providers/mnist_data_provider.py",
  "data_provider_name": "MNIST_DataProvider",

  "platform_constraints": {
    "mode": "auto",
    "auto": {
      "fixed": {
        "target_tq": 16,
        "simulation_steps": 16,
        "weight_bits": 8,
        "allow_axon_tiling": false
      },
      "search_space": {
        "num_core_types": 2,
        "core_type_counts": [200, 200],
        "core_axons_bounds": [64, 1024],
        "core_neurons_bounds": [64, 1024],
        "max_threshold_groups": 3
      }
    }
  },

  "deployment_parameters": {
    "lr": 0.001,
    "training_epochs": 1,
    "tuner_epochs": 1,
    "nas_cycles": 5,
    "nas_batch_size": 5,
    "nas_workers": 1,

    "degradation_tolerance": 0.1,

    "configuration_mode": "nas",
    "model_type": "mlp_mixer",

    "arch_search": {
      "_comment": "Kedi LLM-based optimizer with GPT-4o for better reasoning",
      
      "optimizer": "kedi",
      
      "kedi_model": "openai:gpt-4o",
      "kedi_adapter": "pydantic",
      
      "pop_size": 12,
      "generations": 8,
      "candidates_per_batch": 6,
      "max_regen_rounds": 10,
      "max_failed_examples": 5,
      
      "seed": 42,

      "patch_channels_options": [16, 32, 48, 64, 96, 128, 192, 256],
      "fc_w1_options": [32, 64, 96, 128, 192, 256],
      "fc_w2_options": [32, 64, 96, 128, 192, 256],

      "warmup_fraction": 0.1,
      "training_batch_size": 1024,

      "accuracy_evaluator": "extrapolating",
      "extrapolation_num_train_epochs": 1,
      "extrapolation_num_checkpoints": 5,
      "extrapolation_target_epochs": 10,
      
      "constraints_description": "Multi-objective architecture search for PerceptronMixer on MNIST:\n\nOBJECTIVES:\n- accuracy: MAXIMIZE (higher is better)\n- hard_cores_used: MINIMIZE (fewer hardware cores)\n- avg_unused_area_per_core: MINIMIZE (better utilization)\n- total_params: MINIMIZE (smaller model)\n\nCONFIGURATION SCHEMA:\n{\n  \"model_config\": {\n    \"base_activation\": \"LeakyReLU\",\n    \"patch_n_1\": int (divisor of 28),\n    \"patch_m_1\": int (divisor of 28),\n    \"patch_c_1\": int (from patch_channels_options),\n    \"fc_w_1\": int (from fc_w1_options),\n    \"fc_w_2\": int (from fc_w2_options)\n  },\n  \"platform_constraints\": {\n    \"cores\": [{\"max_axons\": int, \"max_neurons\": int, \"count\": int}, ...],\n    \"max_axons\": int (64-1024, multiple of 8),\n    \"max_neurons\": int (64-1024, multiple of 8),\n    \"target_tq\": 16,\n    \"weight_bits\": 8,\n    \"allow_axon_tiling\": false\n  },\n  \"threshold_groups\": int (1-3)\n}\n\nCONSTRAINTS:\n1. patch_n_1 and patch_m_1 must divide 28 (valid: 1, 2, 4, 7, 14, 28)\n2. Largest layer input must fit within max_axons\n3. Core dimensions must be multiples of 8\n4. Balance accuracy vs hardware efficiency"
    },

    "spike_generation_mode": "Uniform",
    "firing_mode": "Default",
    "thresholding_mode": "<="
  },

  "target_metric_override": null,
  "start_step": "Architecture Search",
  "stop_step": "Architecture Search"
}




